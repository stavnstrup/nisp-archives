<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>

<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>2.4. Human-Computer Interface</title>
      <link rel="stylesheet" href="../style/nc3ta.css" type="text/css"/>
      <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
      <link rel="home" href="index.html" title="NATO Interoperability Standards and Profiles"/>
      <link rel="up" href="ch02.html" title="Chapter 2. Far-Term Emerging Technologies"/>
      <link rel="prev" href="ch02s03.html" title="2.3. Nanotechnology"/>
      <link rel="next" href="ch02s05.html" title="2.5. Portable Power"/>
      <link rel="chapter" href="ch01.html" title="Chapter 1. Introduction"/>
      <link rel="chapter" href="ch02.html" title="Chapter 2. Far-Term Emerging Technologies"/>
      <link rel="chapter" href="ch03.html" title="Chapter 3. Standards"/>
      <link rel="appendix" href="apa.html" title="Appendix A. Acronyms"/>
      <link rel="subsection" href="ch02s04.html#d0e304" title="2.4.1. Hand Controlled Computers"/>
      <link rel="subsection" href="ch02s04.html#d0e321" title="2.4.2. Head Moving Tracking Technology"/>
      <link rel="subsection" href="ch02s04.html#d0e340" title="2.4.3. Eye Tracking Movements"/>
      <link rel="subsection" href="ch02s04.html#d0e354" title="2.4.4. Brain-Computer Interface"/>
      <link rel="subsection" href="ch02s04.html#d0e379" title="2.4.5. Tactile Feedback"/>
      <link rel="subsection" href="ch02s04.html#d0e400" title="2.4.6. Three Dimensional Displays"/>
      <link rel="subsection" href="ch02s04.html#d0e420" title="2.4.7. Automated Language Processing"/>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"/>
      <meta http-equiv="Content-Language" content="en-uk"/>
      <meta name="MSSmartTagsPreventParsing" content="true"/>
      <meta name="author" content="NATO Open Systems Working Group (NOSWG)"/>
      <meta http-equiv="Expires" content="0"/>
   </head>
   <body>
      <div id="taHeader">
         <div id="tophead">
            <span id="quickMenu">
               <a href="../index.html">Home</a> | <a class="noPortal" href="#">Sitemap</a> | 
        <a href="../acronyms/index.html">ABC</a> | <a href="../member.html">Contact</a>
            </span>
         </div>
         <div id="bottomhead">
            <div id="searchBox">
               <form name="form1" method="post" action="">
                  <input name="textfield" type="text" class="invoerVeld"/>
                  <img src="../images/cgey/button_search.gif" alt="Search Button" width="48" height="16"/>
               </form>
            </div>
         </div>
      </div>
      <div id="menubar">
         <a class="noPortal" href="#">Facts</a>
         <a href="../index.html">NISP Volumes</a>
         <a class="noPortal" href="#">Forum</a>
         <a class="noPortal" href="#">NOSWG</a>
      </div>
      <table id="mainbody" cellspacing="0" cellpadding="0">
         <tr>
            <td class="wNavigationBox">
               <div id="taNavigationBox" class="left mainmenu wNavigationBox">
                  <div id="navHeader">NISP Volumes</div>
                  <div id="navMenu">
                     <ul>
                        <li>
                           <a href="../about.html">About the volumes</a>
                        </li>
                        <li class="pdf">
                           <span class="h">Download </span>
                           <a href="../pdf/NISP-Vol4-v4-release.pdf">PDF of V4</a>
                        </li>
                        <li>
                           <a href="../volume1/index.html" title="Introduction and Management">V1 - Introduction and Management</a>
                        </li>
                        <li>
                           <a href="../volume2/index.html" title="Near Term">V2 - Near Term</a>
                        </li>
                        <li>
                           <a href="../volume3/index.html" title="Mid to Far term">V3 - Mid to Far term</a>
                        </li>
                        <li>V4 - Interoperability Profile Guidance<ul id="navSubMenu">
                              <li>1 - <a href="ch01.html">INTRODUCTION</a>
                              </li>
                              <li>2 - <a href="ch02.html">FAR-TERM EMERGING TECHNOLOGIES</a>
                              </li>
                              <li>3 - <a href="ch03.html">STANDARDS</a>
                              </li>
                              <li>A - <a href="apa.html">ACRONYMS</a>
                              </li>
                           </ul>
                        </li>
                        <li>
                           <a href="../volume5-rationale/index.html" title="NISP Design Rules">V5 - NISP Design Rules</a>
                        </li>
                        <li>
                           <a href="../volume6-annexes/index.html" title="NISP - Annexes">V6 - NISP - Annexes</a>
                        </li>
                        <li>
                           <img src="../images/cgey/menu_icon-onder.gif" alt="NATO Logo" width="195" height="72"/>
                        </li>
                     </ul>
                  </div>
               </div>
            </td>
            <td>
               <div id="taContents">
                  <div class="navheader">
                     <table width="100%" summary="Navigation header">
                        <tr>
                           <td width="20%" align="left">
                              <a accesskey="p" href="ch02s03.html">&lt; Prev</a> </td>
                           <th width="60%" align="center"> </th>
                           <td width="20%" align="right"> <a accesskey="n" href="ch02s05.html">Next &gt;</a>
                           </td>
                        </tr>
                     </table>
                     <hr/>
                  </div>
                  <div class="sect1" title="2.4. Human-Computer Interface">
                     <div class="titlepage">
                        <div>
                           <div>
                              <h2 class="title">
                                 <a id="d0e287" style="display:none"> </a>2.4. Human-Computer Interface</h2>
                           </div>
                        </div>
                     </div>
                     <p>49. The idea of eliminating the gap between human thought and computer
      responsiveness is an obvious one, and a number of companies are working
      hard on promising technologies. One of the most obvious such
      technologies is voice recognition software that allows the computer to
      type as you speak, or allows users to control software applications by
      issuing voice commands.</p>
                     <p>50. Even the most advanced and accurate software in this category has
      an accuracy that is impressive, and the technology is far ahead of voice
      recognition technology from a mere decade ago, but it's still not at the
      point where people can walk up to their computer and start issuing voice
      commands without a whole lot of setup, training, and fine tuning of
      microphones and sound levels. Widespread, intuitive use of voice
      recognition technology still appears to be years away.</p>
                     <p>51. And yet our interface with the Internet remains the lowly personal
      computer. With its clumsy interface devices (keyboard and mouse,
      primarily), the personal computer is a makeshift bridge between the
      ideas of human beings and the world of information found on the
      Internet. These interface devices are clumsy and simply cannot keep pace
      with the speed of thought of which the human brain is capable.</p>
                     <p>52. Consider this: a person with an idea who wishes to communicate
      that idea to others must translate that idea into words, then break
      those words into individual letters, then direct her fingers to punch
      physical buttons (the keyboard) corresponding to each of those letters,
      all in the correct sequence. Not surprisingly, typing speed becomes a
      major limiting factor here: most people can only type around sixty words
      per minute. Even a fast typist can barely achieve 120 words per minute.
      Yet the spoken word approaches 300 words per minute, and the speed of
      'thought' is obviously many times faster than that.</p>
                     <p>53. Pushing thoughts through a computer keyboard is sort of like
      trying to put out a raging fire with a garden hose: there is simply not
      enough bandwidth to move things through quickly enough. As a result,
      today's computer / human interface devices are significant obstacles to
      breakthroughs in communicative efficiency.</p>
                     <p>54. The computer mouse is also severely limited. I like to think of
      the mouse as a clumsy translator of intention: if you look at your
      computer screen, and you intend to open a folder, you have to move your
      hand from your keyboard to your mouse, slide the mouse to a new location
      on your desk, watch the mouse pointer move across the screen in an
      approximate mirror of the mouse movement on your desk, then click a
      button twice. Thats a far cry from the idea of simply looking at the
      icon and intending it to open.</p>
                     <p>55. Today's interface devices are little more than rudimentary
      translation tools that allow us to access the world of personal
      computers and the Internet in a clumsy, inefficient way. Still, the
      Internet is so valuable that even these clumsy devices grant us
      immeasurable benefits, but a new generation of computer/human interface
      devices would greatly multiply those benefits and open up a whole new
      world of possibilities for exploiting the power of information and
      knowledge.</p>
                     <div class="sect2" title="2.4.1. Hand Controlled Computers">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e304" style="display:none"> </a>2.4.1. Hand Controlled Computers</h3>
                              </div>
                           </div>
                        </div>
                        <p>56. Another recent technology that represents a clever approach to
        computer / human interfaces is the iGesture Pad by a company called
        Fingerworks (http://www.FingerWorks.com). With the iGesture Pad, users
        place their hands on a touch sensitive pad (about the size of a mouse
        pad), then move their fingers in certain patterns (gestures) that are
        interpreted as application commands. For example, placing your fingers
        on the pad in a tight group, then rapidly opening and spreading your
        fingers are interpreted as an Open command.</p>
                        <p>57. For more intuitive control of software interfaces, what is
        needed is a device that tracks eye movements and accurately translates
        them into mouse movements: so you could just look at an icon on the
        screen and the mouse would instantly move there.</p>
                        <p>58. 
          <span class="bold">
                              <strong>Importance: </strong>
                           </span> This technology
        represents a leap in intuitive interface devices, and it promises a
        whole new dimension of control versus the one-dimensional mouse click.
        Keystrokes and mouse clicks limit a soldier's degree of
        freedom.</p>
                        <p>59. 
          <span class="bold">
                              <strong>Status:</strong>
                           </span> It's still a somewhat
        clumsy translation of intention through physical limbs. Interestingly,
        some of the best technology in this area comes from companies building
        systems for people with physical disabilities. For people who can't
        move their limbs, computer control through alternate means is
        absolutely essential.</p>
                     </div>
                     <div class="sect2" title="2.4.2. Head Moving Tracking Technology">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e321" style="display:none"> </a>2.4.2. Head Moving Tracking Technology</h3>
                              </div>
                           </div>
                        </div>
                        <p>60. One approach to this is tracking the movement of a person's head
        and translating that into mouse movements. One device, the Head Mouse
        (Origin Instruments), does exactly that. You stick a reflective dot on
        your forehead, put the sensor on top of your monitor, and then move
        your head to move your mouse.</p>
                        <p>61. Another company called Madentec (http://www.Madentec.com) offers
        a similar technology called Tracker One. Place a dot on your forehead,
        and then you can control the mouse simply by moving your head.</p>
                        <p>62. In terms of affordable head tracking products for widespread
        use, a company called NaturalPoint (http://www.NaturalPoint.com) seems
        to have the best head tracking technology at the present: a product
        called SmartNav. Add a foot switch and you can click with your
        feet.</p>
                        <p>63. 
          <span class="bold">
                              <strong>Importance: </strong>
                           </span> Allows for
        hands-free control via head movement.</p>
                        <p>64. 
          <span class="bold">
                              <strong>Status:</strong>
                           </span> Current implementations
        present a learning curve for new users, but it works as
        promised.</p>
                     </div>
                     <div class="sect2" title="2.4.3. Eye Tracking Movements">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e340" style="display:none"> </a>2.4.3. Eye Tracking Movements</h3>
                              </div>
                           </div>
                        </div>
                        <p>65. While tracking head movement is in many ways better than
        tracking mouse movement, a more intuitive approach, it seems, would be
        to track actual eye movements. A company called LC Technologies, Inc.
        is doing precisely that with their EyeGaze systems
        (http://www.lctinc.com/products.htm). By mounting one or two cameras
        under your monitor and calibrating the software to your screen
        dimensions, you can control your mouse by simply looking at the
        desired position on the screen.</p>
                        <p>66. This technology was originally developed for people with
        physical disabilities, yet the potential application of it is far
        greater. In time, I believe that eye tracking systems will become the
        preferred method of cursor control for users of personal
        computers.</p>
                        <p>67. Eye tracking technology is quickly emerging as a technology with
        high potential for widespread adoption by the computing public.
        Companies such as Tobii Technology (http://www.tobii.se), Seeing
        Machines (http://www.SeeingMachines.com), SensoMotoric Instruments
        (http://www.smi.de), Arrington Research
        (http://www.ArringtonResearch.com), and EyeTech Digital Systems
        (http://www.eyetechds.com) all offer eye tracking technology with
        potential for computer / human interface applications. The two most
        promising technologies in this list, in terms of widespread
        consumer-level use, appear to be Tobii Technology and EyeTech Digital
        Systems.</p>
                        <p>68. 
          <span class="bold">
                              <strong>Importance: </strong>
                           </span> Allows for
        hands-free control via eye movement.</p>
                     </div>
                     <div class="sect2" title="2.4.4. Brain-Computer Interface">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e354" style="display:none"> </a>2.4.4. Brain-Computer Interface</h3>
                              </div>
                           </div>
                        </div>
                        <p>69. Moving to the next level of human-computer interface technology,
        the ability to control your computer with your thoughts alone seems to
        be an obvious goal. The technology is called Brain Computer Interface
        technology, or BCI.</p>
                        <p>70. Although the idea of brain-controlled computers has been around
        for a while, it received a spike of popularity in 2004 with the
        announcement that nerve-sensing circuitry was implanted in a monkey's
        brain, allowing it to control a robotic arm by merely thinking. Brain
        activity produces electrical signals that are detectable on the scalp
        or cortical surface or within the brain. BCIs translate these signals
        from mere reflections of brain activity into outputs that communicate
        the user&#8217;s intent without the participation of peripheral nerves and
        muscles. BCIs can be non-invasive or invasive. Non-invasive BCIs
        derive the user&#8217;s intent from scalp-recorded electroencephalographic
        (EED) activity. While invasive BCIs derive the user&#8217;s intent from
        neuronal action potentials or local field potentials recorded from
        within the cerebral cortex or from its surface. Researchers have
        studied these systems mainly in nonhuman primates and to a limited
        extent in humans. Invasive BCIs face substantial technical
        difficulties and involve clinical risks. Surgeons must implant the
        recording electrodes in or on the cortex. The devices must function
        well for long periods and they risk infection and may pose other
        damage to the brain.</p>
                        <p>71. 
          <span class="bold">
                              <strong>Importance:</strong>
                           </span> Imagine the
        limitless applications of direct brain control. People could easily
        manipulate cursors on the screen or control electromechanical devices.
        They could direct software applications, enter text on virtual
        keyboards, or even drive vehicles on public roads. Today, all these
        tasks are accomplished by our brains moving our limbs, but the limbs,
        technically speaking, don't have to be part of the chain of
        command.</p>
                        <p>72. 
          <span class="bold">
                              <strong>Status: </strong>
                           </span>The lead researchers in
        the monkey experiment are now involved in a commercial venture to
        develop the technology for use in humans. The company, Cyberkinetics
        Inc. hopes to someday implant circuits in the brains of disabled
        humans and then allow those people to control robotic arms,
        wheelchairs, computers or other devices through nothing more than
        brain behaviour.</p>
                        <div class="figure">
                           <a id="d0e371" style="display:none"> </a>
                           <p/>
                           <div class="figure-contents">
                              <div class="mediaobject">
                                 <img src="../figures/BCI.png" alt="Brain Computer Interface"/>
                              </div>
                           </div>
                           <p class="title">
                              <b>Figure 2.3. Brain Computer Interface</b>
                           </p>
                        </div>
                        <br class="figure-break"/>
                     </div>
                     <div class="sect2" title="2.4.5. Tactile Feedback">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e379" style="display:none"> </a>2.4.5. Tactile Feedback</h3>
                              </div>
                           </div>
                        </div>
                        <p>73. Another promising area of human-computer interface technology is
        being explored by companies like Immersion Corporation
        (http://www.Immersion.com), which offers tactile feedback hardware
        that allows users to 'feel' their computer interfaces.</p>
                        <p>74. Slide on Immersion's CyberGlove, and your computer can track and
        translate detailed hand and finger movements. Add their CyberTouch
        accessory, and tiny force feedback generators mounted on the glove
        deliver the sensation of touch or vibration to your fingers. With
        proper software translation, these technologies give users the ability
        to manipulate virtual objects using their hands. It's an intuitive way
        to manipulate objects in virtual space, since nearly all humans have
        the natural ability to perform complex hand movements with practically
        no training whatsoever.</p>
                        <p>75. Another company exploring the world of tactile feedback
        technologies is SensAble Technologies. Their PHANTOM devices allow
        users to construct and feel three-dimensional objects in virtual
        space. Their consumer-level products include a utility for gamers that
        translate computer game events into tactile feedback (vibrations,
        hitting objects, gun recoil, etc.).</p>
                        <p>76. On a consumer level, Logitech makes a device called the IFeel
        Mouse that vibrates or thumps when your mouse cursor passes over
        certain on-screen features. Clickable icons, for example, feel like
        bumps as you mouse over them. The edges of windows can also deliver
        subtle feedback.</p>
                        <p>77. 
          <span class="bold">
                              <strong>Importance:</strong>
                           </span> Key technology for
        modeling &amp; simulation, and simulated training. Tactile feedback
        has potential for making human-computer interfaces more intuitive and
        efficient;even if today's tactile technologies are clunky first
        attempts. The more senses we can directly involve in our control of
        computers, the broader the bandwidth of information and intention
        between human beings and machines.</p>
                        <p>78. 
          <span class="bold">
                              <strong>Status: </strong>
                           </span> Hasn't seen much
        success in the marketplace.</p>
                     </div>
                     <div class="sect2" title="2.4.6. Three Dimensional Displays">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e400" style="display:none"> </a>2.4.6. Three Dimensional Displays</h3>
                              </div>
                           </div>
                        </div>
                        <p>79. The long-promised 3D computer monitor finally seems to be close
        to reality. Manipulating complex windows, documents and virtual
        objects on a two-dimensional display -- as is standard today -- is
        rather limiting. With a 3D monitor, we could work in layers or
        position documents and objects in 3D space rather than squeezing them
        down to a tiny toolbar at the bottom of one screen.</p>
                        <p>80. For human beings, 3D space is intuitive. We get it without
        training. That's because we live in a world of 3D objects and space,
        and our perception is hard-wired to understand spatial relationships.
        That's why gamers who play first-person shooters like Quake can
        mentally retrace their way through enormous maps (levels) in their
        heads, eyes closed, without even trying: the human brain was built to
        remember and navigate 3D space.</p>
                        <p>81. Recent breakthroughs in 3D display promise to make computing
        more intuitive and powerful. Companies like LightSpace Technologies
        (http://www.lightspacetech.com) are already selling desktop 3D display
        monitors that display true 3D images without the need for special
        glasses.</p>
                        <p>82. The trouble is, Windows and Mac operating systems weren't
        written with 3D displays in mind. So there's no capability to stack
        windows or view the depth of objects. It's a classic chicken-and-egg
        conundrum: who's going to buy 3D displays if the software can't
        support them, and why would software makers write 3D layering logic if
        nobody owns the displays?</p>
                        <p>83. In time, thanks to the cool factor of 3D displays, the
        technology will eventually receive enough attention to warrant the
        necessary R&amp;D investment by operating system developers like
        Microsoft and Apple. No doubt, future generations will conduct all
        their computing with the aid of 3D displays, and the very idea of 2D
        displays will seem as outdated as black &amp; white movies do to us
        today.</p>
                        <p>84. Another new 3D display device is the Perspecta Spatial 3D globe,
        seen at: http://www.actuality-systems.com/index.php/actuality. This
        device displays 3D objects or animations inside a globe. Users can
        walk around the globe and view the objects from any angle. It's a
        rather expensive item, of course, so early applications for this
        product focus on medical and research tasks. In time, however, the
        technology will drop in price, bringing it within reach of more
        consumers.</p>
                        <p>85. 
          <span class="bold">
                              <strong>Importance:</strong>
                           </span> For that, we will
        ultimately need a tabletop 3D display system that lays flat on your
        desk (like an LCD monitor laying down) and projects 3D images into the
        space above the panel. This would be a true volumetric 3D display
        system, and it's here that the technology truly represents a
        breakthrough. Program application windows could literally be stacked
        from the rear to the front, and if you peeked around the side of the
        display, you could see a side view of all the windows at once. With
        proper software control, objects or documents could be placed in true
        3D space: desktop icons, for example, could be lined up along the very
        back row. Games could display true 3D scenes as if you're actually in
        them, and CAD engineers would have the ability to observe their
        designs in true 3D space. Better yet, if coupled with a motion
        tracking glove or similar technology, users could use their hands to
        grasp, move, resize or otherwise manipulate elements in 3D space.
        This, of course, opens up an unlimited universe of possibilities for
        computer / human interaction.</p>
                     </div>
                     <div class="sect2" title="2.4.7. Automated Language Processing">
                        <div class="titlepage">
                           <div>
                              <div>
                                 <h3 class="title">
                                    <a id="d0e420" style="display:none"> </a>2.4.7. Automated Language Processing</h3>
                              </div>
                           </div>
                        </div>
                        <p>86. Foreign language speech and text are indispensable sources of
        intelligence, but the vast majority of this information is unexamined.
        Foreign language data and their corresponding providers are massive
        and growing in numbers daily. Moreover, because the time to transcribe
        and translate foreign documents is so labor intensive, compounded by
        the lack of linguists with suitable language skills to review it all,
        much foreign language speech and text are never exploited for
        intelligence and counterterrorism purposes. New and powerful foreign
        language technology is needed to allow English-speaking analysts to
        exploit and understand vastly more foreign speech and text than is
        currently possible today.</p>
                        <div class="figure">
                           <a id="d0e425" style="display:none"> </a>
                           <p/>
                           <div class="figure-contents">
                              <div class="mediaobject">
                                 <img src="../figures/LanguageProcessing.png" alt="Language Processing"/>
                              </div>
                           </div>
                           <p class="title">
                              <b>Figure 2.4. Language Processing</b>
                           </p>
                        </div>
                        <br class="figure-break"/>
                        <div class="sect3" title="2.4.7.1. Speech-to-Text Transcription">
                           <div class="titlepage">
                              <div>
                                 <div>
                                    <h4 class="title">
                                       <a id="d0e433" style="display:none"> </a>2.4.7.1. Speech-to-Text Transcription</h4>
                                 </div>
                              </div>
                           </div>
                           <p>87. Automatic speech-to-text transcription seeks to produce rich,
          readable transcripts of foreign news broadcasts and conversations
          (over noisy channels and/or in noisy environments) despite
          widely-varying pronunciations, speaking styles, and subject matter.
          Goals for speech-to-text transcription include: (1) providing high
          accuracy multilingual word-level transcription from speech at all
          stages of processing and across multiple genres, topics, speakers,
          and channels ( such as, Arabic, Chinese, the Web, news, blogs,
          signals intelligence, and databases); (2) representing and
          extracting &#8220;meaning&#8221; out of spoken language by reconciling and
          resolving jargon, slang, code-speak, and language ambiguities; (3)
          dynamically adapting to (noisy) acoustics, speakers, topics, new
          names, speaking-styles, and dialects; (4) improving relevance to
          deliver the information decision-makers need; (5) assimilating and
          integrating speech across multiple sources to support exploration
          and analysis to enable natural queries and drill-down; and (6)
          increased portability across languages, sources, and information
          needs.</p>
                           <p>88. 
            <span class="bold">
                                 <strong>Importance:</strong>
                              </span> Examples of
          critical technologies include: improved acoustic modeling; robust
          feature extraction; better discriminative estimation models;
          improved language and pronunciation modeling; and language
          independent approaches that are able to learn from examples by using
          algorithms that exploit advances in computational power plus the
          large quantities of electronic speech and text that are now
          available. The ultimate goal is to create rapid, robust technology
          that cab be ported cheaply and easily to other languages and
          domains.</p>
                        </div>
                        <div class="sect3" title="2.4.7.2. Foreign-to-English Translation">
                           <div class="titlepage">
                              <div>
                                 <div>
                                    <h4 class="title">
                                       <a id="d0e443" style="display:none"> </a>2.4.7.2. Foreign-to-English Translation</h4>
                                 </div>
                              </div>
                           </div>
                           <p>89. Goals for foreign to English translation include: (1)
          providing high accuracy machine translation and structural metadata
          annotation from multilingual text document and speech transcription
          input at all stages of processing and across multiple genres,
          topics, and mediums ( such as, Arabic, Chinese, the Web, news,
          blogs, signals intelligence, and databases); (2) understanding or at
          least deriving semantic intent from input strings regardless of
          source; (3) reconciling and resolving semantic differences,
          duplications, inconsistencies, and ambiguities across words,
          passages, and documents; (4) more efficient discovery of important
          documents, more relevant and accurate facts while decreasing the
          amount of time required to do it, and passages for distillation; (5)
          providing enriched translation output that is formatted, cleaned-up,
          clear, unambiguous, and meaningful to decision-makers; (6)
          eliminating the need for human intervention and minimized delay of
          information delivery; and (7) fast development of new language
          capability, swift response to breaking events, and increased
          portability across languages, sources, and information needs.</p>
                           <p>90. 
            <span class="bold">
                                 <strong>Importance:</strong>
                              </span> Examples of
          critical technologies include: improved dynamic language modeling
          with adaptive learning; advanced machine translation technology that
          utilizes heterogeneous knowledge sources; better inference models;
          language-independent approaches to create rapid robust technology
          that can be ported cheaply and easily to any language and domain;
          syntactic and semantic representation techniques to deal with
          ambiguous meaning and information overload; and cross- and mono-
          lingual, language-independent information retrieval to detect and
          discover the exact data in any language quickly and accurately, and
          to flag new data that may be of interest.</p>
                        </div>
                     </div>
                  </div>
                  <div class="navfooter">
                     <hr/>
                     <table width="100%" summary="Navigation footer">
                        <tr>
                           <td width="40%" align="left">
                              <a accesskey="p" href="ch02s03.html">&lt; Prev</a> </td>
                           <td width="20%" align="center">
                              <a accesskey="u" href="ch02.html">Up</a>
                           </td>
                           <td width="40%" align="right"> <a accesskey="n" href="ch02s05.html">Next &gt;</a>
                           </td>
                        </tr>
                        <tr>
                           <td width="40%" align="left" valign="top"> </td>
                           <td width="20%" align="center">
                              <a accesskey="h" href="index.html">Home</a>
                           </td>
                           <td width="40%" align="right" valign="top"> </td>
                        </tr>
                     </table>
                  </div>
               </div>
            </td>
         </tr>
      </table>
      <div id="taFooter">Copyright © NATO - OTAN 1998-2010 | <a href="../disclaimer.html">Disclaimer</a>
      </div>
   </body>
</html>